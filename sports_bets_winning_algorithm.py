# -*- coding: utf-8 -*-
"""sports-bets-winning-algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mu1EiFxr8dt0hfIkJPBEuwno0G3MPfc6

# Sports Bets Winning Algorithm
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

stadiums = pd.read_csv("/kaggle/input/nfl-scores-and-betting-data/nfl_stadiums.csv",header=0,encoding='unicode_escape')
teams = pd.read_csv("/kaggle/input/nfl-scores-and-betting-data/nfl_teams.csv",header=0)
scores = pd.read_csv("/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv",header=0)


#stadiums['stadium_weather_station_code'] = stadiums['stadium_weather_station_code'].astype('float')
#stadiums['stadium_capacity'] = stadiums['stadium_capacity'].astype('float')
stadiums['LATITUDE'] = stadiums['LATITUDE'].astype('float')
stadiums['LONGITUDE'] = stadiums['LONGITUDE'].astype('float')
stadiums['ELEVATION'] = stadiums['ELEVATION'].astype('float')


scores['schedule_date'] = scores['schedule_date'].astype('datetime64[ns]')
scores['schedule_season'] = scores['schedule_season'].astype('datetime64[ns]').dt.year
scores['weather_temperature'] = scores['weather_temperature'].astype('float')
scores['score_home'] = scores['score_home'].astype('float')
scores['score_away'] = scores['score_away'].astype('float')
scores['weather_temperature'] = scores['weather_temperature'].astype('float')
scores['weather_wind_mph'] = scores['weather_wind_mph'].astype('float')
scores['weather_humidity'] = scores['weather_humidity'].astype('float')
scores['schedule_season'] = scores['schedule_season'].astype('float')

# Create a winners list.
winner =[]
# Obtain the scores for each area
for i,v in scores.score_home.items():
    if scores.score_home[i]>scores.score_away[i]:
        winner.append(scores.team_home[i])

    elif scores.score_home[i]<scores.score_away[i]:
        winner.append(scores.team_away[i])
    else:
        winner.append('Tie')

scores['winner'] = winner

"""# What does our data look like?"""

# show sample of dataset
scores.sample(25).sort_values('schedule_date',ascending=False).reset_index(drop=True)

"""# And we have a winner!"""

scores[['team_home','score_home','score_away','team_away','winner']].sample(25)

"""# Atlanta Falcons - Wins
Correlation map.
"""

from string import ascii_letters
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

atlanta = scores[scores.team_home == 'Atlanta Falcons'].sort_values('schedule_date',ascending=False).reset_index(drop=True)

sns.set_theme(style="white")


# Compute the correlation matrix
corr = atlanta.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.xticks(rotation=45)
plt.show()

"""### Correlation map of factors when Falcons win."""

# When do the Falcons win?

atlanta_wins = atlanta[atlanta.score_home > atlanta.score_away]

from string import ascii_letters
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="white")


# Compute the correlation matrix
corr = atlanta_wins.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.xticks(rotation=45)
plt.show()

"""# Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams["xtick.labelsize"] = 6
# plot a sample of 100 observations that lasted under 60 minutes
# need to get a smaller sample of city-set, the x-axis is way too muddled.
sns.catplot(data=atlanta_wins.sample(100), x="schedule_date",
            y="score_home",
            hue="team_away",
            kind="swarm",
            height=10,
            aspect=2,
            #size='score_away',
            #size_max=20
           )
plt.xticks(rotation=45)
plt.ylim(bottom=0)
plt.show()

import plotly.express as px
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)

fig = px.scatter_3d(scores, z='team_home', y='score_home', x='stadium',
              color='stadium',
              size = 'score_home',
              symbol = 'winner',
              #hover_name = 'shape',
              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],
              opacity=0.7,
              size_max=25

                   )
fig.show()

import plotly.express as px
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)

fig = px.scatter_3d(atlanta_wins, z='schedule_date', y='score_home', x='stadium',
              color='stadium',
              size = 'score_home',
              #symbol = 'schedule_playoff',
              #hover_name = 'shape',
              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],
              opacity=0.7,
              size_max=25

                   )
fig.show()

import plotly.express as px
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)

fig = px.scatter_3d(scores, z='schedule_date', y='team_home', x='stadium',
              color='team_away',
              size = 'score_home',
              #symbol = 'stadium',
              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],
              opacity=0.7,
              size_max=25

                   )
fig.show()

"""# Predictive Modeling - Who will win?"""

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix

# encoding
from sklearn.preprocessing import LabelEncoder

def encode(df):
    lb_make = LabelEncoder()
    columns = df.columns.values.tolist()
    df_encoded = df[columns].copy()

    # categorize/encode
    for i in columns:
        df_encoded[i] = lb_make.fit_transform(df[i])

    # encoded
    return df_encoded

# create X,y variables for ML
from sklearn.model_selection import train_test_split
def X_y_sets(df, target):
    X = df.dropna().drop(columns=[target]).copy()
    y = df.dropna()[target].ravel().copy()

    return train_test_split(X, y, test_size=0.33, random_state=42), X, y


# encoded variable re-mapping
def encoding_remap(df, df_encoded, target):

    X_test = X_y_sets(df, target)[0][0]

    remap = pd.merge(df_encoded.loc[df_encoded.index.isin(X_test.index.values)][target].reset_index(),
              df.loc[df.index.isin(X_test.index.values)][[target]].reset_index(),on=['index'])

    remap[target] = [str(remap[f'{target}_y'][i]) for i,v in remap[f'{target}_x'].items()]
    remap['index'] = np.array([str(remap[f'{target}_x'][i]) for i,v in remap[f'{target}_x'].items()]).astype(int)
    remap=remap[[target,'index']]
    remap = remap.set_index('index').drop_duplicates().sort_values('index')

    return remap


# pairplot
import seaborn as sns
def pairplot(df, target):
    return sns.pairplot(df,hue=target)

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier

# classifier iteration
def classification_feat_importance(df_encoded):

    # iterate through each column variable as classification targets
    for target in df_encoded.columns.values:
        X = df_encoded.drop(columns=[target]).copy()
        y = df_encoded[target].ravel().copy()

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)


        # classifiers
        #clf1 = GradientBoostingClassifier(criterion="friedman_mse", init=None, learning_rate=0.3338, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)
        #clf2 = GradientBoostingClassifier(criterion="squared_error", init=None, learning_rate=0.2222, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)
        clf3 = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=42).fit(X_train, y_train)
        clf4 = ExtraTreesClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)
        clf5 = AdaBoostClassifier(n_estimators=8000, random_state=42).fit(X_train, y_train)
        clf6 = MLPClassifier(alpha=1, max_iter=500).fit(X_train, y_train)
        clf7 = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)
        classifiers = [
                       #clf1,
                       #clf2,
                       clf3,
                       clf4,
                       clf5,
                       clf6,
                       #clf7
                      ]

        for classifier in classifiers:
            results = []
            results.append({"classifier":str(classifier).split("(")[0],"target":target,"test_score":classifier.score(X_test, y_test)})
            for i in results:
                if target == 'verified':
                    print("\nClassifier:",str(classifier).split("(")[0],"\nTarget:",target,"\nScore:",classifier.score(X_test, y_test))

        test_matrix = confusion_matrix(y_test, clf.predict(X_test))
        results = pd.DataFrame(results)

    return results,test_matrix

print("To analyze which target-classifier would yield the best results: \nUncomment (#) the code below.")

# is scaling necessary?
# construction of ML dataframes
target = 'winner'

# copy
a = scores.copy()

# for the sake of computationa efficiency
a = a

# find random sample & save index for defining an encoded use-case
from random import randrange
idx = randrange(len(a))

# print random configuration item
print("\nThis is a randomly chosen subject we will try to predict.")
b = pd.DataFrame(a.loc[idx]).T
print(f"\nTarget:'{target}' value is: ",b.reset_index()[target][0],"\n")

# store sol'n
solution = str(b.reset_index()[target][0])

# print data point
b
# if this cell fails, try it again from step 1 - you ran into a null variable (i'll fix that soon enough)

# categorize/encode entire dataframe(a)
c = encode(a)
print("\nOriginal dataframe encoded into something we can run a classifier against.\n")
c.sample(10).reset_index(drop=True).style.background_gradient(cmap ='Pastel1').set_properties(**{'font-size': '10px'})

# 'comments' & 'country' - out
sns.pairplot(c.copy(),
             hue=f'{target}',
             kind="kde",
             corner=True,
             palette="Paired"
            )

# print encoded item
use_case = pd.DataFrame(c.loc[idx]).T.drop(columns=[target])

#c

# print encoded item w/out target info
data = c.drop(columns=[target,'score_home','score_away'])

print("\nThis is what our encoded 'use-case' looks like - number form, just the way the machine likes it.\n")

use_case.style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})

"""## Here we train the machine using previous scores, weather, stadium, & the over_under_lines variables.
### However, we do remove the score_home & score_away variables from our use-case because we don't want the machine to be godly omniscient.
"""

# create X,y variables for ML
# save trainer
print("\nResetting train data...\nCreating X-matrix & y-vector (target) for classification.")
trainer = c.loc[c.index!=idx].copy()
X, y =  trainer.drop(columns=[target,'score_home','score_away']), trainer[target].ravel()
X_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]

X_train['target'] = pd.Series(y_train)
X_train.dropna().head().reset_index(drop=True).reset_index(drop=True).style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})

# for the sake of adding the 'target' column above for sake of layman's explanation
X_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]

# encoded variable re-mapping
# specific to our current target choice
d = encoding_remap(a, c, target)
print("\nDecoding our encoded dataframe to correlate with the initial randomly chosen subject.\n")

print("\n-Live prediction-\nThinking...\n")

# choose classifier
#clf = GradientBoostingClassifier(criterion="friedman_mse", init=None, learning_rate=0.3338, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)
#clf = GradientBoostingClassifier(criterion="squared_error", init=None, learning_rate=0.2222, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)

# these ones run just a little more efficiently for now
#clf = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=42).fit(X_train, y_train)
#clf = ExtraTreesClassifier(n_estimators=1000, random_state=42).fit(X_train, y_train)
#clf = AdaBoostClassifier(n_estimators=1500, random_state=42).fit(X_train, y_train)
clf = MLPClassifier(alpha=0.666, max_iter=666).fit(X_train, y_train)
#clf = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)

print()
print("Test score (confidence): ",clf.score(X_test, y_test)*100,"%")
print()
prediction = clf.predict(use_case)[0]
print(f"Prediction {target} index:",prediction)

# print decoded prediction
print("\nPrediction Decoded")
e = d[d.index == prediction]
e

"""# Was our prediction model successful?"""

solved = str(e.winner[e.index[0]])
if solution == solved:
    print(f"\nYUP!\n\nThe machine correctly predicted the '{target}'!\n")
else:
    print("\nNOPE!\nThe machine's prediction was incorrect :(")

print()